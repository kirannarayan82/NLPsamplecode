{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "textdata_train=pd.read_csv(r'C:\\Users\\genus\\OneDrive\\Documents\\flights\\NLP\\train.csv')\n",
    "textdata_test=pd.read_csv(r'C:\\Users\\genus\\OneDrive\\Documents\\flights\\NLP\\test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\genus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\genus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#from gensim.models import KeyedVectors\n",
    "import keras \n",
    "from keras.models import Sequential, Model \n",
    "from keras import layers\n",
    "from keras.layers import Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'textdata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_65512/1046497770.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtextdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'textdata' is not defined"
     ]
    }
   ],
   "source": [
    "textdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stop words\n",
    "stopwords=nltk.corpus.stopwords.words('English')\n",
    "ps=nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    import string\n",
    "    text=\"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    text=\"\".join([word.lower() for word in text if word not in string.digits])\n",
    "     \n",
    "    token=re.split(\"\\W+\",text)\n",
    "    text=[ps.stem(word) for word in token if word not in stopwords]\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdata_train['text']=textdata_train['text'].apply(lambda x:clean_text(x))\n",
    "textdata_test['text']=textdata_test['text'].apply(lambda x:clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdata_train['text']=[text for text in textdata['text'] if text not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect=TfidfVectorizer()\n",
    "tfidf_vect_fit=tfidf_vect.fit(textdata_train['text'])\n",
    "tfidftext=tfidf_vect_fit.transform(textdata_train['text'])\n",
    "tfidftext_forpredict=tfidf_vect_fit.transform(textdata_test['text'])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(tfidftext,textdata_train['target'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 21637)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidftext_forpredict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1523, 21637)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import  TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6090, 21637)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1523, 21637)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1523, 21637)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestClassifier(n_estimators=100,max_depth=None,n_jobs=1)\n",
    "rfmodel=rf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=rfmodel.predict(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionstarget=rfmodel.predict(tfidftext_forpredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(predictionstarget).to_csv('nlppredictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision,recall,fscore,train_support=score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'precision' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_65512/4215214920.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrecall\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'precision' is not defined"
     ]
    }
   ],
   "source": [
    "precision,recall,fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>874</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>625</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0     0   1\n",
       "target         \n",
       "0       874   4\n",
       "1       625  20"
      ]
     },
     "execution_count": 685,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73538073, 0.05979073])"
      ]
     },
     "execution_count": 686,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features=1000\n",
    "maxlen=10\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [deed, reason, earthquak, may, allah, forgiv, us]\n",
       "1            [forest, fire, near, la, rong, sask, canada]\n",
       "2       [resid, ask, shelter, place, notifi, offic, ev...\n",
       "3       [, peopl, receiv, wildfir, evacu, order, calif...\n",
       "4       [got, sent, photo, rubi, alaska, smoke, wildfi...\n",
       "                              ...                        \n",
       "7608    [two, giant, crane, hold, bridg, collaps, near...\n",
       "7609    [ariaahrari, thetawniest, control, wild, fire,...\n",
       "7610           [utckm, volcano, hawaii, httptcozdtoydebj]\n",
       "7611    [polic, investig, ebik, collid, car, littl, po...\n",
       "7612    [latest, home, raze, northern, california, wil...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdata['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test=train_test_split(textdata_train['text'],textdata_train['target'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3542    #Russia 'food crematoria' provoke outrage in c...\n",
       "624                            To fight bioterrorism sir.\n",
       "7424    I'm liable to sound like a wounded animal duri...\n",
       "3004    @RetiredFilth people in sydney woke up to the ...\n",
       "758     Max blew tf up ! ?????? shots fired ???? #Catf...\n",
       "                              ...                        \n",
       "4803    @ToxicSavior_ -a loud bang. He froze on the sp...\n",
       "5873                      I'll ruin my life if I have to.\n",
       "3224    Emergency Shutdown Systems - Edmonton http://t...\n",
       "5428    Police: Gunman reported dead at Nashville area...\n",
       "5467    #Reddit updates #content #policy promises to q...\n",
       "Name: text, Length: 6090, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizetext(text):\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from nltk import word_tokenize\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    max_features=1000\n",
    "    maxlen=10\n",
    "    batch_size=32\n",
    "    #tokens = word_tokenize(text)\n",
    "    MAX_LEN=50\n",
    "    tokenizer_obj=Tokenizer()\n",
    "    tokenizer_obj.fit_on_texts(text)\n",
    "    sequences=tokenizer_obj.texts_to_sequences(text)\n",
    "    sequences=np.array(sequences)\n",
    "    sequences=sequences.squeeze()\n",
    "    sequences=sequences.flatten()\n",
    "    sequences=sequences.reshape(1,-1)\n",
    "    return(sequences.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearnestedlist(text):      \n",
    "    newlist=[]\n",
    "    for element in text:\n",
    "      if (isinstance(element,list)):\n",
    "        newlist.append(element[0])\n",
    "      else:\n",
    "        newlist.append(element)\n",
    "    return(newlist)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mylist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_65512/493011507.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnewlist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmylist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mnewlist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mylist' is not defined"
     ]
    }
   ],
   "source": [
    "newlist=[]\n",
    "for element in mylist:\n",
    "    if (isinstance(element,list)):\n",
    "        newlist.append(element[0])\n",
    "    else:\n",
    "        newlist.append(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(textdata_train['text'])\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "training_sequences = tokenizer.texts_to_sequences(textdata_train['text'])\n",
    "pad_training = pad_sequences(training_sequences, maxlen=25, padding='post', truncating='post')\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(textdata_test['text'])\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "test_sequences = tokenizer.texts_to_sequences(textdata_test['text'])\n",
    "pad_testing = pad_sequences(test_sequences, maxlen=25, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 25)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 25)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_testing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras import Model,Sequential\n",
    "from keras.layers import Embedding,Flatten\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.add(Embedding(10000,8,input_length=25))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model.add(LSTM(10))\n",
    "model.add((Dense(1,activation='sigmoid')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7/7 [==============================] - 1s 28ms/step - loss: 0.6915 - acc: 0.5529 - val_loss: 0.6906 - val_acc: 0.5597\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.6880 - acc: 0.5981 - val_loss: 0.6884 - val_acc: 0.5796\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6848 - acc: 0.6228 - val_loss: 0.6864 - val_acc: 0.5874\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.6817 - acc: 0.6355 - val_loss: 0.6845 - val_acc: 0.5957\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.6788 - acc: 0.6434 - val_loss: 0.6828 - val_acc: 0.5969\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6760 - acc: 0.6475 - val_loss: 0.6812 - val_acc: 0.5979\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.6734 - acc: 0.6509 - val_loss: 0.6798 - val_acc: 0.5989\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6708 - acc: 0.6528 - val_loss: 0.6785 - val_acc: 0.6000\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6683 - acc: 0.6539 - val_loss: 0.6772 - val_acc: 0.6018\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.6661 - acc: 0.6553 - val_loss: 0.6760 - val_acc: 0.6067\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6639 - acc: 0.6572 - val_loss: 0.6750 - val_acc: 0.6063\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6617 - acc: 0.6581 - val_loss: 0.6740 - val_acc: 0.6057\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6596 - acc: 0.6589 - val_loss: 0.6732 - val_acc: 0.6059\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.6576 - acc: 0.6595 - val_loss: 0.6723 - val_acc: 0.6056\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.6557 - acc: 0.6601 - val_loss: 0.6716 - val_acc: 0.6064\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.6539 - acc: 0.6605 - val_loss: 0.6709 - val_acc: 0.6065\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6521 - acc: 0.6610 - val_loss: 0.6703 - val_acc: 0.6066\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6503 - acc: 0.6614 - val_loss: 0.6696 - val_acc: 0.6068\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.6487 - acc: 0.6616 - val_loss: 0.6689 - val_acc: 0.6064\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6470 - acc: 0.6619 - val_loss: 0.6681 - val_acc: 0.6064\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6454 - acc: 0.6621 - val_loss: 0.6674 - val_acc: 0.6065\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6438 - acc: 0.6624 - val_loss: 0.6667 - val_acc: 0.6072\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6423 - acc: 0.6626 - val_loss: 0.6662 - val_acc: 0.6073\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6408 - acc: 0.6626 - val_loss: 0.6655 - val_acc: 0.6075\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6393 - acc: 0.6628 - val_loss: 0.6649 - val_acc: 0.6076\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.6378 - acc: 0.6628 - val_loss: 0.6644 - val_acc: 0.6070\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6364 - acc: 0.6630 - val_loss: 0.6638 - val_acc: 0.6071\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6350 - acc: 0.6630 - val_loss: 0.6632 - val_acc: 0.6077\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.6337 - acc: 0.6631 - val_loss: 0.6629 - val_acc: 0.6077\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.6323 - acc: 0.6631 - val_loss: 0.6627 - val_acc: 0.6076\n",
      "Epoch 31/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.6311 - acc: 0.6632 - val_loss: 0.6625 - val_acc: 0.6076\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6298 - acc: 0.6632 - val_loss: 0.6620 - val_acc: 0.6077\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.6286 - acc: 0.6633 - val_loss: 0.6616 - val_acc: 0.6077\n",
      "Epoch 34/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6274 - acc: 0.6633 - val_loss: 0.6611 - val_acc: 0.6077\n",
      "Epoch 35/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6263 - acc: 0.6633 - val_loss: 0.6606 - val_acc: 0.6081\n",
      "Epoch 36/50\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.6251 - acc: 0.6633 - val_loss: 0.6603 - val_acc: 0.6081\n",
      "Epoch 37/50\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6240 - acc: 0.6634 - val_loss: 0.6602 - val_acc: 0.6078\n",
      "Epoch 38/50\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6230 - acc: 0.6634 - val_loss: 0.6601 - val_acc: 0.6079\n",
      "Epoch 39/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6219 - acc: 0.6635 - val_loss: 0.6597 - val_acc: 0.6078\n",
      "Epoch 40/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6210 - acc: 0.6635 - val_loss: 0.6594 - val_acc: 0.6081\n",
      "Epoch 41/50\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.6200 - acc: 0.6634 - val_loss: 0.6592 - val_acc: 0.6082\n",
      "Epoch 42/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.6191 - acc: 0.6635 - val_loss: 0.6591 - val_acc: 0.6080\n",
      "Epoch 43/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6182 - acc: 0.6635 - val_loss: 0.6591 - val_acc: 0.6077\n",
      "Epoch 44/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6173 - acc: 0.6635 - val_loss: 0.6589 - val_acc: 0.6087\n",
      "Epoch 45/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6165 - acc: 0.6636 - val_loss: 0.6588 - val_acc: 0.6085\n",
      "Epoch 46/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.6158 - acc: 0.6636 - val_loss: 0.6586 - val_acc: 0.6079\n",
      "Epoch 47/50\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.6150 - acc: 0.6635 - val_loss: 0.6586 - val_acc: 0.6082\n",
      "Epoch 48/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.6142 - acc: 0.6635 - val_loss: 0.6586 - val_acc: 0.6074\n",
      "Epoch 49/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.6135 - acc: 0.6635 - val_loss: 0.6588 - val_acc: 0.6072\n",
      "Epoch 50/50\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.6127 - acc: 0.6635 - val_loss: 0.6591 - val_acc: 0.6077\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x220f2372e80>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(pad_training,textdata_train['target'],epochs=50,batch_size=1000,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(pad_testing)\n",
    "finaltarget = (prediction>0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81570</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81571</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81572</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81573</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81574</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81575 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0      0\n",
       "1      1\n",
       "2      0\n",
       "3      1\n",
       "4      1\n",
       "...   ..\n",
       "81570  0\n",
       "81571  0\n",
       "81572  0\n",
       "81573  0\n",
       "81574  0\n",
       "\n",
       "[81575 rows x 1 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(finaltarget.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newlist=[]\n",
    "for element in mylist:\n",
    "    if (isinstance(element,list)):\n",
    "        newlist.append(element[0])\n",
    "    else:\n",
    "        newlist.append(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ccd8363866b194dbcde3adf1ee698e1426409fef5a044f95017085037d7b2eb3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
